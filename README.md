# Paper_Survey

# Computer Vision
|Year|Journal/Conference|Title|Reviewer|Links|
|:-:|:-:|:-|:-:|:-:|
|2021|arXiv|Masked-attention Mask Transformer for Universal Image Segmentation |김현일|[Paper](https://arxiv.org/pdf/2112.01527.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/10)|
|2021|nips|Per-Pixel Classification is Not All You Need for Semantic Segmentation |김현일|[Paper](https://arxiv.org/pdf/2107.06278.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/9)|
|2021|arXiv|A Survey of Visual Transformers |김현일|[Paper](https://arxiv.org/pdf/2111.06091.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/8)|
|2021|CVPR|Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers |김현일|[Paper](https://arxiv.org/abs/2012.15840), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/5)|
|2021|ICCV (Spotlight)|LAMBDANETWORKS: MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION|김현일|[Paper](https://openreview.net/pdf?id=xTJEN-ggl1b), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/3)|
|2021|arXiv|MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER|김현일|[Paper](https://arxiv.org/abs/2110.02178), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/2)|
|2021|arXiv|ResNet strikes back: An improved training procedure in timm|김현일|[Paper](https://arxiv.org/abs/2110.00476), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/1)|


# Knowledge Distillation
|Year|Journal/Conference|Title|Reviewer|Links|
|:-:|:-:|:-|:-:|:-:|
2021|NIPS|Does Knowledge Distillation Really Work? |김현일|[Paper](https://papers.nips.cc/paper/2021/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/14) [ppt](https://github.com/gusdlf93/Paper_Survey/files/8791484/Knowledge.Distillation.pdf)|
|2021|arXiv|Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results |김현일|[Paper](https://arxiv.org/pdf/2204.03475.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/15) [ppt](https://github.com/gusdlf93/Paper_Survey/files/8791484/Knowledge.Distillation.pdf)|
|2021|IJCV| Knowledge Distillation: A Survey |김현일|[Paper](https://arxiv.org/pdf/2006.05525.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/13) [ppt](https://github.com/gusdlf93/Paper_Survey/files/8791484/Knowledge.Distillation.pdf)|

# Loss Function
|Year|Journal/Conference|Title|Reviewer|Links|
|:-:|:-:|:-|:-:|:-:|
|2019|CVPR|ArcFace: Additive Angular Margin Loss for Deep Face Recognition |김현일|[Paper](https://arxiv.org/pdf/1801.07698.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/7)|

# Self-Supervised Learning
|Year|Journal/Conference|Title|Reviewer|Links|
|:-:|:-:|:-|:-:|:-:|
|2021|arxiv|Masked Autoencoders Are Scalable Vision Learners|김현일|[Paper](https://arxiv.org/pdf/2111.06377.pdf), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/6) [ppt] (https://github.com/gusdlf93/Paper_Survey/files/7658356/Masked.autoencoders.are.scalable.vision.learners.pptx)|

# Medical
|Year|Journal/Conference|Title|Reviewer|Links|
|:-:|:-:|:-|:-:|:-:|
|2022|Deep Bio ppt| Stain Normalization Survey |김현일|[ppt](https://docs.google.com/presentation/d/1SR5xTKa0x5CpZckJayaA_BmRpTAqdvp6svjj7_zsQFc/edit?usp=sharing), [Summary](https://github.com/gusdlf93/Paper_Survey/issues/12)|
